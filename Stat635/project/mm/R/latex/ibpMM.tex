\documentclass[mathserif]{article}
\usepackage{fullpage}
\usepackage{pgffor}
\usepackage{amssymb}
\usepackage{bm}
\usepackage{mathtools}
\usepackage{verbatim}
\usepackage{appendix}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage[UKenglish]{isodate} % for: \today
\cleanlookdateon                % for: \today
\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}

\def\wl{\par \vspace{\baselineskip}\noindent}
\def\beginmyfig{\begin{figure}[htbp]\begin{center}}
\def\endmyfig{\end{center}\end{figure}}
\def\prodl#1#2#3{\prod\limits_{#1=#2}^{#3}}
\def\suml#1#2#3{\sum\limits_{#1=#2}^{#3}}
\def\ds{\displaystyle}
\def\tbf#1{\textbf{#1}}
\def\inv{^{\raisebox{.2ex}{$\scriptscriptstyle-1$}}}
\def\pm{^{\raisebox{.2ex}{$\scriptscriptstyle\prime$}}}
\newcommand{\m}[1]{\mathbf{\bm{#1}}} % Serif bold math

\begin{document}
% my title:
\begin{center}
  \section*{\textbf{Using the Indian Buffet Process to Estimate the Design Matrix Z in Random Intercept Mixed Models}
    \footnote{https://github.com/luiarthur/Fall2014/blob/master/Stat635/project}
  }
  \subsection*{\textbf{Arthur Lui}}
  \subsection*{\noindent\today}
\end{center}


\section*{Motivating Example}
Often, the data we observe are generated by latent features. Clustering
algorithms can be used to group these observations as a means of exploratory
analysis in preparation for further analysis. In Figure 1, we see a scatter
plot of a simulated dataset. The data were generated with a linear model with a
constant slope by three different intercepts. Hence, we observe three clusters
of data, each appear to have a linear trend, with the same slope, but different
intercepts.  Each cluster is composed of 30 points, and make up a total of 90
observations.  The figure in the middle is the true clustering of the data. The
right-most figure is the clustering we obtain using the k-means algorithm and
setting the number of clusters to three. We can see that the k-means algorithm
does very well in clustering the observations, but does not always classify
correctly. One reason is that no distribution is used to inform the k-means
algorithm, when there is actually a linear trend for each cluster. Using the
Indian buffet process (IBP) a Bayesian non-parametric distribution for binary
matrices of infinite dimensions, I will propose a model for obtaining the true
clustering of the data.

  \beginmyfig
    \includegraphics[scale=.3]{images/scatter.pdf}
    \includegraphics[scale=.3]{images/clus.pdf}
    \includegraphics[scale=.3]{images/kmean.pdf}
    \vspace{-5mm}
    \caption{Simulated Observations. Scatter plot (Left). True clustering (Middle). Clustering
             obtained using k-means (Right).}
  \endmyfig


\section*{Review of Mixed Models}
Linear mixed models (LMM) are used when researchers want to account for the
random effects that exists between groups of observations in a linear model.
Each group may be repeated mearuements on the same subject. An LMM can be
expressed in mathematical terms as
\[
  \m{y = X\beta + Z\gamma + \epsilon},
\]
where $\m{y}$ is a vector of responses, $\m{X}$ are the covariates, $\m{Z}$ is
the block design matrix, and $\m{\beta \text{ and } \gamma}$ are the coefficients for $\m{X}$ and $\m{Z}$ respectively.


\section*{The IBP}
One key problem in recovering the latent structure responsible for generating
observed data is determining the number of latent features. The Indian Buffet
process (IBP) provides a flexible distribution for sparse binary matrices with
infinite dimensions (i.e. finite number of rows, and infinite number of
columns).  When used as a prior distribution in a latent feature model, the IBP
can learn the number of latent features generating the observations because it
can draw binary matrices which have a potentially infinite number of columns.
We will use the IBP as a prior distribution in a Gaussian latent feature model
to recover the latent structures generating the observations.\\

\noindent
The IBP is a distribution for sparse binary matrices with a finite number of
rows and potentially an infinite number of columns. The process of generating a
realization from the IBP can be described by an analogy involving Indian buffet
restaurants.\\

\noindent
Let $Z$ be an $N \times \infty$ binary matrix. Each row in $Z$ represents a
customer who enters an Indian buffet and each column represents a dish in the
buffet. Customers enter the restaurant one after another. The first customer
samples an $r=$Poisson$(\alpha)$ number of dishes, where $\alpha > 0$ is a mass
parameter which influences the final number of sampled dishes. This is
indicated in by setting the first r columns of the first row in $Z$ to be $1$.
The other values in the row are set to $0$. Each subsequent customer samples
each previously sampled dish with probability proportional to its popularity.
That is, the next customer samples dish $k$ with probability $m_k/i$,
where $m_k$ is the number of customers that sampled dish $k$, and $i$ is the
current customer number (or row number in $Z$). Each customer also samples an
additional Poisson$(\alpha/i)$ number of new dishes. Once all the $N$ customers
have gone through this process, the resulting $Z$ matrix will be a draw from
the Indian buffet process with mass parameter $\alpha$. In other words, $Z \sim
\text{IBP}(\alpha)$. Note that $\alpha \propto K_+$, where $K_+$ is the final
number of sampled dishes (occupied columns). Figure 2.1 shows a draw from an
IBP(10) with 50 rows. The white squares are 1, indicating that a dish was
taken; black squares are 0, indicating that a dish was not taken. \\
\beginmyfig
  % Reverse Colors
  \includegraphics[scale=.4]{images/ibpExample.pdf}
  \caption{Two random draws from the Indian buffet process with 5 rows
           and $\alpha=2$ (Left), and $\alpha=5$ (Right). The expected number of new
           dishes for each customer, and consequently the number of non-zero columns in an
           IBP draw increases with $\alpha$.}
\endmyfig

\noindent
The probability of any particular matrix produced from this process is
\begin{equation}
  P(\bm{Z}) = \frac{\alpha^{K_+}}{\prodl{i}{1}{N} {K_1}^{(i)}!} 
              \exp\{-\alpha H_N\}\prodl{k}{1}{K_+}
              \frac{(N-m_k)!(m_k-1)!}{N!},
\end{equation}
where $H_N$ is the harmonic number, $\suml{i}{1}{N}\ds\frac{1}{i}$, $K_+$ is
the number of non-zero columns in $\bm Z$, $m_k$ is the $k^{th}$ column sum of
$\bm Z$, and $K_1^{(i)}$ is the ``number of new dishes" sampled by customer $i$.\\


\subsection*{Gibbs Sampler for Indian Buffet Process}
One way to get a draw from the IBP($\alpha$) is to simulate the process
according to the description above. Another way is to implement a Gibbs
sampler. We can implement a Gibbs sampler to draw from the IBP as follows:

\begin{enumerate}
  \item Start with an arbitrary binary matrix of $N$ rows
  \item For each row, $i$,
  \begin{enumerate}
    \item For each column, $k$,
    \item if $m_{-i,k}$ = $0$, delete column $k$. Otherwise,
    \item set $z_{ik}$ to $0$
    \item set $z_{ik}$ to $1$ with probability $P(z_{ik}=1|\bm{z_{-i,k}}) = \frac{m_{-i,k}}{i}$
    \item at the end of row $i$, add Poisson($\ds\frac{\alpha}{N}$) columns of $1$'s
  \end{enumerate}
  \item iterate step 2 a large number of times
\end{enumerate}
We can likewise incorporate this Gibbs sampler to sample from the posterior
distribution P($\bm{Z|X}$) where $\bm Z \sim \text{IBP}(\alpha)$ by
sampling from the complete conditional
\begin{equation}
  P(z_{ik}=1|\bm{Z_{-(ik)},X})  \propto p(\bm{X|Z}) P(z_{ik}=1|\bm{Z_{-(ik)}}).
\end{equation}

\noindent
The parameter $\alpha$ is often unknown, so it should be modeled. Note that the
conjugate prior for $\alpha$ is a Gamma distribution. Using a Gamma
distribution is appropriate since $\alpha$ is positive.
\[
  \begin{array}{rcl}
    \bm Z|\alpha & \sim & \text{IBP}(\alpha)\\
          \alpha & \sim & \text{Gamma}(a,b), \text{where $b$ is the scale parameter}\\
    & & \\
    p(\alpha|\bm Z) & \propto & p(\bm Z|\alpha) p(\alpha)\\
    p(\alpha|\bm Z) & \propto & \alpha^{K_+} e^{-\alpha H_N}  
                                \alpha^{a-1} e^{-\alpha/b}\\
    p(\alpha|\bm Z) & \propto & \alpha^{a+K_+-1} e^{-\alpha(1/b+H_N)}\\
  \end{array}
\]
\begin{equation}
  \alpha|\bm Z  \sim  \text{Gamma}(a+K_+, (1/b+H_N)^{-1})
\end{equation}

\section*{Using the IBP as a Prior for Z}
If we assume $\m{y}$ has a multivariate normal likelihood, then $\m{y \sim }$
MVN($\m{X\beta+Z\gamma},\sigma_\epsilon^2$). We can also give normal priors to
$\m{\beta}$ and $\m{\gamma}$. $\m{Z}$ will have an IBP($ \alpha$) prior. For
the hyper parameter $\alpha$, we can use a Gamma prior to make use of Gibbs
sampling. Succinctly,
\begin{align*}
  \m{y} &\sim \text{MVN}(\m{X\beta+Z\gamma},\sigma_\epsilon^2)\\
  \m{Z} &\sim \text{IBP}(\alpha)\\
  \alpha &\sim \text{Gamma}(1,1)\\
  \sigma_\epsilon^2 &\sim \text{InverseGamma}(1,\text{rate}=5)\\
\end{align*}

\begin{align*}
  \m{[y|Z,X]} &= \int_\beta\int_\gamma 
                   \m{[y|Z,X,\gamma,\beta][\gamma][\beta]}
                 d\gamma d\beta
\end{align*}

\section*{Mathematics}
\section*{Simulation Study}
  \beginmyfig
    \includegraphics[scale=.7]{images/agpost.pdf}
  \endmyfig  

  \beginmyfig
    \includegraphics[scale=.4]{images/EZpre.pdf}
    \includegraphics[scale=.4]{images/EZ.pdf}
    \vspace{-5mm}
    \caption{Estimated Z matrix (Left). Reparameterized estimated Z matrix (Right).}
  \endmyfig  



  \beginmyfig
    \includegraphics[scale=.4]{images/KZ.pdf}
    \vspace{-5mm}
    \caption{Z Matrix (K-Means)}
  \endmyfig  

  \beginmyfig
    \includegraphics[scale=.3]{images/truemm.pdf}
    \includegraphics[scale=.3]{images/ibpmm.pdf}
    \includegraphics[scale=.3]{images/KMmm.pdf}
    \vspace{-5mm}
  \endmyfig


  \input{images/mb.tex}
  \input{images/mg.tex}     

\section*{Future Work}
Difficult to integrate out parameters if likelihood is NOT Normal.
Also difficult to come up with proposal mechanism, but easier. 
We will come up with proposal mechanism in the summer.

\end{document}
